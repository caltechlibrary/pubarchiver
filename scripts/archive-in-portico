#!/bin/bash
# =============================================================================
# @file    archive-in-portico
# @brief   Script meant to be run from cron
# @author  Michael Hucka <mhucka@caltech.edu>
# @license Please see the file named LICENSE in the project directory
# @website https://github.com/caltechlibrary/pubarchiver
#
# Principles:
#
#  1. The frequency of archives is set by the cron schedule, not in here.
#
#  2. The local output directory where files are saved is set by environment
#     variable PORTICO_OUTPUT.
#
#  3. A timestamp file is created in the output directory, and pubarchiver
#     is run with that date stamp as the "after date" argument.
#
#  4. Archives are created in subdirectories of the output directory named
#     after the date+time they were done.  E.g., "2019-08-29-1000".
#
#  5. A report file is written to the output directory and named "report.csv".
#
#  6. A log file is written to the output directory and named "run.log".
#
#  7. Successful results are uploaded to ftp.portico.org using ftp credentials
#     that are passed via environment variables PORTICO_USER and PORTICO_PASS.
#
#  8. Outcomes are emailed to the comma-separated addresses in environment
#     variable $EMAIL_SUCCESS or $EMAIL_FAILURE, depending on whether
#     execution succeed or failed, respectively.  They are also posted to the
#     Slack channel set by environment vaiable SLACK_CHANNEL using API token
#     SLACK_CLI_TOKEN.  (Note: SLACK_CLI_TOKEN is read by the command
#     `slack`, which is why it does not appear used below.)
#
# =============================================================================

# Store the path to this script so we can report it in error messages.
mainscript=${BASH_SOURCE[0]}

# Files recording the results of the most recent run are stored at the level
# of the directory indicatd by $PORTICO_OUTPUT, because this information is
# carried across runs.
today=$(date +%Y-%m-%d)
datestampfile=$PORTICO_OUTPUT/last-run-date
failurefile=$PORTICO_OUTPUT/last-failures

# Today's run will be written in a subdirectory.  Note the subdirectory name
# includes the current time, not just today's date, because otherwise we
# would overwrite the previous data if we ran run multiple times per day.
now=$(date +%Y-%m-%d-%H%M)
outputdir=$PORTICO_OUTPUT/$now

# Each run produces several log and record files in $outputdir.
log=$outputdir/run.log
report=$outputdir/report.csv
debuglog=$outputdir/debug.log


# Set up Python environment ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

CWD="$(cd -P -- "$(dirname -- "$mainscript")" && pwd -P)"
source $CWD/env/bin/activate


# Read helper functions ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

source $CWD/helpers.sh


# Do the real work ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Before doing anything else, create the output directory.
mkdir -p $outputdir

# Start by recording the current time.
echo "Starting at $now" >> $log
echo "" >> $log

# Read the date stamp from the previous run.
if [[ -e $datestampfile ]]; then
    read afterdate < $datestampfile
    # Go back one day to include the run date in the argument given to
    # pubarchiver's --after-date (or else we miss the day of the run itself).
    afterdate=$(date -d "$afterdate -1 days" +"%Y-%m-%d")
else
    # We've never run, or someone reset the date stamp.
    # Use a fake date that basically signifies "since forever".
    afterdate="1900-01-01"
fi

# Run pubarchiver separately on past failures, leaving the results unpackaged
# so that we can add to them the results of today's run.
rerun_count=0
if [[ -f $failurefile ]]; then
    echo "=== Running pubarchiver on past failures ===" >> $log
    echo "" >> $log
    # Note the use of -Z to prevent zip'ing the final results.
    run_pubarchiver -j micropublication -d portico -Z -C -f $failurefile \
                     -o $outputdir -r $report -s csv,html \
                     -t Past_failures_retried -@ $debuglog
    echo "" >> $log
fi
mv $outputdir/report.html $outputdir/rerun-report.html

echo "=== Running pubarchiver for new articles ===" >> $log
echo "" >> $log
this_report=$outputdir/latest-report.csv
this_debuglog=$outputdir/latest-debug.log
# This will add new articles to any existing ones from the past failures
# code above, and this time will zip up the final result.
run_pubarchiver -j micropublication -d portico -C -a $afterdate -o $outputdir \
                -r $this_report -s csv,html -t $today -@ $this_debuglog

# Combine separate report files, leave that, & delete the intermediate files.
tail -n +2 $this_report >> $report
tail $this_debuglog >> $debuglog
rm -f $this_debuglog $this_report

# Did we have any failures? If so, note them for next time.
grep -i "missing," $outputdir/*report.csv | cut -f2 -d',' > $failurefile

# If we downloaded new articles, ftp the archive to Portico.
# Note #1: the file redirection is to avoid wc printing the file name.
# Note #2: the -gt 1 is to skip the header line when counting the lines.
lines=$(wc -l < $report)
if [[ $lines -gt 1 ]]; then
    archivefile=$outputdir/micropublication-org.zip
    echo "" >> $log
    echo "=== FTP'ing file using curl ===" >> $log
    curl -T $archivefile ftp://ftp.portico.org \
        --user $PORTICO_USER:$PORTICO_PASS >> $log 2>&1
fi

# If we get this far, write out a date stamp file to indicate that things
# ran successfully and to give the next run a starting point.
echo $today > $datestampfile


# Mail the report ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

grep -F "Total articles" $log | \
    sed 's/Total //g;1 s/articles left after filtering/Past failures retried/;2 s/ left after filtering//g;2 s/articles/New &/' | \
    mail -s "Portico archiving results for $today" \
         -a $outputdir/latest-report.html -a $outputdir/rerun-report.html \
         -a $log $EMAIL_SUCCESS


# Post the report to Slack ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

run_slack chat send --channel $SLACK_CHANNEL --color "#00ff00" \
    --title "Portico run for micropublications.org completed." \
    --text "There were $(wc -l < $failurefile) articles skipped."
